{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtrMbTVMo82Tlq8eV8g1sK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElianaHolanda/Rede-Neural-do-Zero/blob/main/Rede_Neural_do_Zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZRzXBIMho2s"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "wcojEyMGltFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms. ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST(\"./MNIST_data/\", download=True, train=False, transform=transform) # Carrega a parte de validação \n",
        "valloader = torch.utils.data.DataLoader (valset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n"
      ],
      "metadata": {
        "id": "GmI7IWjBlv6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vhoa-DHAp0rO",
        "outputId": "5a629524-ed90-4607-d601-cdcf7508197b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conferindo tamanho da imagem\n",
        "print(imagens[0].shape)\n",
        "#Conferindo tamanho da etiqueta\n",
        "print(etiquetas[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBlLYNRNxyuj",
        "outputId": "776427dd-007e-4368-ac5e-40dde8686e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__() \n",
        "    self.linear1 = nn.Linear (28*28, 128) # camada de entrada, 784 neurônios que se ligam a 128 \n",
        "    self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn. Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10\n",
        "    # para a camada de saida não e necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self,x):\n",
        "    X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1 \n",
        "    X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3 (X) # função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x \n",
        "    return F.log_softmax(x, dim=1) # dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "6FET6mHC1bJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a polítca de atualização dos pesos e da bias inicio time() # timer para sabermos quanto tempo levou o treino\n",
        "  criterio = nn. NLLLoss() # definindo o criterio para calcular a perda \n",
        "  EPOCHS=10 # numero de epochs que o algoritmo rodará\n",
        "  modelo.train() # ativando o modo de treinamento do modelo\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para \"vetores de 28*28 casas para fi \n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio (output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada"
      ],
      "metadata": {
        "id": "Lrf8FU4h2aDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device): \n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img= imagens[i].view(1, 784)\n",
        "      #desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento \n",
        "      with torch.no_grad():\n",
        "        logps = modelo (img.to(device)) # output do modelo em escala logaritmica\n",
        "\n",
        "      ps = torch.exp(logps) # converte output para escala normal (lembrando que é um tensor) \n",
        "      probab= list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index (max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu \n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto \n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"Total de imagens testadas\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas *100/conta_todas))"
      ],
      "metadata": {
        "id": "FJbMqu3p4vGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbWZlC-I9Rj1",
        "outputId": "23dc909d-5143-48c4-81dd-3f16de528ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWuWjk8G9znH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}